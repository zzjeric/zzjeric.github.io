<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Eric's blog"><title>基于kubeadm搭建k8s集群v1.16 | Eric's blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">基于kubeadm搭建k8s集群v1.16</h1><a id="logo" href="/.">Eric's blog</a><p class="description">生如逆旅 一苇以航</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">基于kubeadm搭建k8s集群v1.16</h1><div class="post-meta">2020-02-01<span> | </span><span class="category"><a href="/categories/Technology/">Technology</a></span></div><a class="disqus-comment-count" href="/2020/02/01/k8s_cluster_deploy_116/#vcomment"><span class="valine-comment-count" data-xid="/2020/02/01/k8s_cluster_deploy_116/"></span><span> 条评论</span></a><div class="post-content"><p>Kubeadm是k8s官方提供的一个k8s集群搭建和管理工具，能简化很多配置工作。</p>
<p>参考： <a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm/">https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm/</a></p>
<h3><span id="1-软件版本">1 软件版本：</span></h3><ul>
<li>Kubeadm：</li>
<li>K8s：v1.16</li>
<li>Os： Centos 7.x</li>
<li>Kernel： Linux 5.3.7-1.el7.elrepo.x86_64</li>
</ul>
<h3><span id="2-配置虚拟机环境">2  配置虚拟机环境：</span></h3><h5><span id="21-检查centos内核版本所有节点">2.1 检查centos内核版本（所有节点）：</span></h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# cat /etc/redhat-release</span><br><span class="line">CentOS Linux release 7.2.1511 (Core) </span><br><span class="line">[root@localhost ~]# uname -sr</span><br><span class="line">Linux 3.10.0-327.el7.x86_64</span><br></pre></td></tr></table></figure>

<p>可以看到用的centos7.2，内核是3.x的，版本有点低，如果需要启用ipvs，需要升级内核到最新版本（或者使用Centos 7.6、7.8等比较新的os），升级后内核版本为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# uname -sr</span><br><span class="line">Linux 5.3.7-1.el7.elrepo.x86_64</span><br></pre></td></tr></table></figure>

<p>升级centos内核：</p>
<p>在 CentOS 7 上启用 ELRepo 仓库：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org</span><br><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm</span><br></pre></td></tr></table></figure>

<p>使用下面的命令列出可用的内核相关包：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum --disablerepo=&quot;*&quot; --enablerepo=&quot;elrepo-kernel&quot; list available</span><br></pre></td></tr></table></figure>

<p>安装最新的主线稳定内核：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum --enablerepo=elrepo-kernel install kernel-ml</span><br></pre></td></tr></table></figure>

<p>设置grub默认内核版本：</p>
<p>编辑/etc/default/grub 文件，内容替换为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GRUB_TIMEOUT=5</span><br><span class="line">GRUB_DEFAULT=0</span><br><span class="line">GRUB_DISABLE_SUBMENU=true</span><br><span class="line">GRUB_TERMINAL_OUTPUT=&quot;console&quot;</span><br><span class="line">GRUB_CMDLINE_LINUX=&quot;rd.lvm.lv=centos/root rd.lvm.lv=centos/swap crashkernel=auto rhgb quiet&quot;</span><br><span class="line">GRUB_DISABLE_RECOVERY=&quot;true&quot;</span><br></pre></td></tr></table></figure>

<p>运行下面的命令来重新创建内核配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grub2-mkconfig -o /boot/grub2/grub.cfg</span><br></pre></td></tr></table></figure>

<p>然后reboot重启，完成后，用uname -sr重新查看内核版本是否已升级。</p>
<p>参考：<a target="_blank" rel="noopener" href="https://linux.cn/article-8310-1.html">https://linux.cn/article-8310-1.html</a></p>
<h5><span id="22-配置ntp-服务器所有节点">2.2 配置NTP 服务器（所有节点）</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo yum -y install ntp</span><br><span class="line">systemctl start ntpd</span><br><span class="line">https://blog.csdn.net/zhangjie0412/article/details/77935584</span><br></pre></td></tr></table></figure>

<h5><span id="23-配置hosts所有节点">2.3 配置hosts（所有节点）</span></h5><p>在/所有节点的etc/hosts文件中，配置各个节点的ip<br><code>cat /etc/hosts</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">172.18.10.18 node1</span><br><span class="line">172.18.10.19 node2</span><br><span class="line">172.18.10.20 node3</span><br></pre></td></tr></table></figure>

<p>其中node1作为master节点，node2和node3作为普通节点。</p>
<h5><span id="24-修改或禁用防火墙开放端口所有节点">2.4 修改或禁用防火墙开放端口（所有节点）</span></h5><p>如果各个主机启用了防火墙，需要开放Kubernetes各个组件所需要的端口，可以查看Installing kubeadm中的”Check required ports”一节。 简单起见在各节点禁用防火墙：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure>

<p>生产环境下，这里需要考虑一下。</p>
<p>禁用SELINUX：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">setenforce 0</span><br><span class="line">vi /etc/selinux/config</span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure>

<p>创建/etc/sysctl.d/k8s.conf文件，添加如下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure>

<p>执行命令使修改生效。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">modprobe br_netfilter</span><br><span class="line">sysctl -p /etc/sysctl.d/k8s.conf</span><br></pre></td></tr></table></figure>

<h5><span id="25-配置ipvs所有节点">2.5 配置ipvs（所有节点）</span></h5><p>kube-proxy开启ipvs 需要加载一些内核模块，在5.3.7的内核版本，nf_conntrack_ipv4被nf_contrack代替，所以使用下面的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line">#!/bin/bash</span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack</span><br><span class="line">EOF</span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack</span><br></pre></td></tr></table></figure>

<p>各个节点安装ipset和ipvsadm：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install ipset</span><br><span class="line">yum install ipvsadm</span><br></pre></td></tr></table></figure>

<h5><span id="26-安装docker所有节点">2.6 安装docker（所有节点）</span></h5><p>安装docker的yum源:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager \</span><br><span class="line">    --add-repo \</span><br><span class="line">https://download.docker.com/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure>

<p>查看可用的docker版本：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum list docker-ce.x86_64  --showduplicates |sort -r</span><br></pre></td></tr></table></figure>

<p>在各节点安装docker最新的的19.03.4版本。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">yum makecache fast</span><br><span class="line"></span><br><span class="line">yum install -y --setopt=obsoletes=0 \</span><br><span class="line">  docker-ce-19.03.4-3.el7</span><br><span class="line"></span><br><span class="line">systemctl start docker</span><br><span class="line">systemctl enable docker</span><br></pre></td></tr></table></figure>

<p>确认一下iptables filter表中FOWARD链的默认策略(pllicy)为ACCEPT。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -nvL</span><br></pre></td></tr></table></figure>
<p>修改docker cgroup driver为systemd:</p>
<p>创建或修改/etc/docker/daemon.json：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>重启docker：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>

<p>用下面的命令验证一下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker info | grep Cgroup</span><br><span class="line">Cgroup Driver: system</span><br></pre></td></tr></table></figure>

<h5><span id="27-关闭swap所有节点">2.7 关闭swap（所有节点）</span></h5><p>Kubernetes 1.8开始要求关闭系统的Swap，如果不关闭，默认配置下kubelet将无法启动</p>
<p>关闭系统的Swap方法如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">swapoff -a</span><br></pre></td></tr></table></figure>

<p>修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改/etc/sysctl.d/k8s.conf添加下面一行：</p>
<p>vm.swappiness=0</p>
<p>执行sysctl -p /etc/sysctl.d/k8s.conf使修改生效。</p>
<h5><span id="28-安装kubeadm和kubelet所有节点">2.8 安装kubeadm和kubelet（所有节点）</span></h5><p>配置repo：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg</span><br><span class="line">        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>安装：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum makecache fast</span><br><span class="line">yum install -y kubelet kubeadm kubectl</span><br></pre></td></tr></table></figure>

<p>在各节点开机启动kubelet服务：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable kubelet.service</span><br></pre></td></tr></table></figure>

<h3><span id="3-使用kubeadm部署k8s">3 使用kubeadm部署k8s</span></h3><h5><span id="31-部署master节点">3.1 部署master节点</span></h5><p>3.1.1 使用kubeadm init 初始化master 节点<br>使用kubeadm config print init-defaults可以打印集群初始化默认的使用的配置：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">================</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta2</span></span><br><span class="line"><span class="attr">bootstrapTokens:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">groups:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">system:bootstrappers:kubeadm:default-node-token</span></span><br><span class="line"><span class="attr">token:</span> <span class="string">abcdef.0123456789abcdef</span></span><br><span class="line"><span class="attr">ttl:</span> <span class="string">24h0m0s</span></span><br><span class="line"><span class="attr">usages:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">signing</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">authentication</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">InitConfiguration</span></span><br><span class="line"><span class="attr">localAPIEndpoint:</span></span><br><span class="line"><span class="attr">advertiseAddress:</span> <span class="number">1.2</span><span class="number">.3</span><span class="number">.4</span></span><br><span class="line"><span class="attr">bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">nodeRegistration:</span></span><br><span class="line"><span class="attr">criSocket:</span> <span class="string">/var/run/dockershim.sock</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">node1</span></span><br><span class="line"><span class="attr">taints:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiServer:</span></span><br><span class="line"><span class="attr">timeoutForControlPlane:</span> <span class="string">4m0s</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta2</span></span><br><span class="line"><span class="attr">certificatesDir:</span> <span class="string">/etc/kubernetes/pki</span></span><br><span class="line"><span class="attr">clusterName:</span> <span class="string">kubernetes</span></span><br><span class="line"><span class="attr">controllerManager:</span> &#123;&#125;</span><br><span class="line"><span class="attr">dns:</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">CoreDNS</span></span><br><span class="line"><span class="attr">etcd:</span></span><br><span class="line"><span class="attr">local:</span></span><br><span class="line"><span class="attr">dataDir:</span> <span class="string">/var/lib/etcd</span></span><br><span class="line"><span class="attr">imageRepository:</span> <span class="string">k8s.gcr.io</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterConfiguration</span></span><br><span class="line"><span class="attr">kubernetesVersion:</span> <span class="string">v1.16.0</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line"><span class="attr">dnsDomain:</span> <span class="string">cluster.local</span></span><br><span class="line"><span class="attr">serviceSubnet:</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.0</span><span class="string">/12</span></span><br><span class="line"><span class="attr">scheduler:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>从默认的配置中可以看到，可以使用imageRepository定制在集群初始化时拉取k8s所需镜像的地址。基于默认配置定制出本次使用kubeadm初始化集群所需的配置文件kubeadm.yaml：<br>（注意修改advertiseAddress和kubernetsversion，nodesubet可以自己定义）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">advertiseAddress: 172.18.10.18</span><br><span class="line">bindPort: 6443</span><br><span class="line">nodeRegistration:</span><br><span class="line">taints:</span><br><span class="line">- effect: PreferNoSchedule</span><br><span class="line">key: node-role.kubernetes.io/master</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.16.0</span><br><span class="line">networking:</span><br><span class="line">podSubnet: 10.244.0.0/16</span><br></pre></td></tr></table></figure>

<p>使用kubeadm默认配置初始化的集群，会在master节点打上node-role.kubernetes.io/master:NoSchedule的污点，阻止master节点接受调度运行工作负载。这里测试环境只有两个节点，所以将这个taint修改为node-role.kubernetes.io/master:PreferNoSchedule，意思是pod会优先调度到普通节点，普通节点不够用master节点也可以运行业务pod。</p>
<p>参考： <a target="_blank" rel="noopener" href="https://blog.frognew.com/2018/05/taint-and-toleration.html">https://blog.frognew.com/2018/05/taint-and-toleration.html</a></p>
<p>在开始初始化集群之前可以使用kubeadm config images pull预先在各个节点上拉取所k8s需要的docker镜像。</p>
<p>接下来使用kubeadm初始化集群，选择node1作为Master Node，在node1上执行下面的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --config kubeadm.yaml --ignore-preflight-errors=Swap</span><br></pre></td></tr></table></figure>

<p>执行成功后，会打印出这段说明：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 172.18.10.18:6443 --token ynmb9s.8darvwxj0y6klgj4 \</span><br><span class="line">--discovery-token-ca-cert-hash sha256:345eeb7602eb6133ad032c8515f1d4a7a0d4180759893b32ddf1c043c9a73770 </span><br></pre></td></tr></table></figure>

<p>这里面提示要执行下面命令生成配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure>

<p>（只在master节点执行即可）</p>
<p>另外给了 普通node节点加入集群的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 172.18.10.18:6443 --token ynmb9s.8darvwxj0y6klgj4 \</span><br><span class="line">--discovery-token-ca-cert-hash sha256:345eeb7602eb6133ad032c8515f1d4a7a0d4180759893b32ddf1c043c9a73770 </span><br></pre></td></tr></table></figure>

<p>查看一下集群状态，确认个组件都处于healthy状态：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get cs</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">NAME                 STATUS    MESSAGE             ERROR</span><br><span class="line">controller-manager   Healthy   ok                  </span><br><span class="line">scheduler            Healthy   ok                  </span><br><span class="line">etcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>集群初始化如果遇到问题，可以使用下面的命令进行清理：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubeadm reset</span><br><span class="line">ifconfig cni0 down</span><br><span class="line">ip link delete cni0</span><br><span class="line">ifconfig flannel.1 down</span><br><span class="line">ip link delete flannel.1</span><br><span class="line">rm -rf /var/lib/cni/</span><br></pre></td></tr></table></figure>

<p>这些清理命令同样适用于从节点，清理需谨慎。</p>
<p>3.1.2 在主节点安装flannel network add-on 网络插件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/k8s/</span><br><span class="line">cd ~/k8s</span><br><span class="line">curl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br><span class="line">kubectl apply -f  kube-flannel.yml</span><br></pre></td></tr></table></figure>

<p>成功后打印：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">clusterrole.rbac.authorization.k8s.io/flannel created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/flannel created</span><br><span class="line">serviceaccount/flannel created</span><br><span class="line">configmap/kube-flannel-cfg created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-amd64 created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-arm64 created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-arm created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-ppc64le created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-s390x created</span><br></pre></td></tr></table></figure>

<p>如果Node有多个网卡的话，参考lannel issues 39701，目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上–iface=<iface-name></iface-name></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">containers:</span><br><span class="line">      - name: kube-flannel</span><br><span class="line">        image: quay.io/coreos/flannel:v0.11.0-amd64</span><br><span class="line">        command:</span><br><span class="line">        - /opt/bin/flanneld</span><br><span class="line">        args:</span><br><span class="line">        - --ip-masq</span><br><span class="line">        - --kube-subnet-mgr</span><br><span class="line">        - --iface=eth1</span><br></pre></td></tr></table></figure>

<p>使用kubectl get pod –all-namespaces -o wide确保所有的Pod都处于Running状态。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -n kube-system</span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-5c98db65d4-dr8lf        1/1     Running   0          52m</span><br><span class="line">coredns-5c98db65d4-lp8dg        1/1     Running   0          52m</span><br><span class="line">etcd-node1                      1/1     Running   0          51m</span><br><span class="line">kube-apiserver-node1            1/1     Running   0          51m</span><br><span class="line">kube-controller-manager-node1   1/1     Running   0          51m</span><br><span class="line">kube-flannel-ds-amd64-mm296     1/1     Running   0          44s</span><br><span class="line">kube-proxy-kchkf                1/1     Running   0          52m</span><br><span class="line">kube-scheduler-node1            1/1     Running   0          51m</span><br></pre></td></tr></table></figure>

<p>测试集群DNS是否可用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run curl --image=radial/busyboxplus:curl -it</span><br></pre></td></tr></table></figure>

<p>进入后执行nslookup kubernetes.default确认解析正常:</p>
<p>nslookup kubernetes.default</p>
<h5><span id="32-将普通node节点加入集群">3.2 将普通node节点加入集群</span></h5><p>很简单，只需要在普通node节点，执行上面给出的join命令即可（注意一定要参考2.7节关闭swap）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 172.18.10.18:6443 --token ynmb9s.8darvwxj0y6klgj4 \</span><br><span class="line">--discovery-token-ca-cert-hash sha256:345eeb7602eb6133ad032c8515f1d4a7a0d4180759893b32ddf1c043c9a73770 </span><br></pre></td></tr></table></figure>

<p>在master节点通过kubectl get node 查看：</p>
<h5><span id="33-怎样从集群中移除node">3.3 怎样从集群中移除node</span></h5><p>如果需要从集群中移除node2这个Node执行下面的命令：</p>
<p>在master节点上执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl drain node2 --delete-local-data --force --ignore-daemonsets</span><br><span class="line">kubectl delete node node2</span><br></pre></td></tr></table></figure>

<p>在node2上执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubeadm reset</span><br><span class="line">ifconfig cni0 down</span><br><span class="line">ip link delete cni0</span><br><span class="line">ifconfig flannel.1 down</span><br><span class="line">ip link delete flannel.1</span><br><span class="line">rm -rf /var/lib/cni/</span><br></pre></td></tr></table></figure>

<p>在node1上执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete node node2</span><br></pre></td></tr></table></figure>

<h3><span id="4-安装后配置">4 安装后配置</span></h3><h5><span id="41-kube-proxy开启ipvs">4.1 kube-proxy开启ipvs</span></h5><p>在master节点，修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs”</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit cm kube-proxy -n kube-system</span><br></pre></td></tr></table></figure>

<p>之后重启各个节点上的kube-proxy pod，在master节点执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -n kube-system | grep kube-proxy | awk &#x27;&#123;system(&quot;kubectl delete pod &quot;$1&quot; -n kube-system&quot;)&#125;&#x27;</span><br></pre></td></tr></table></figure>

<p>运行下面的命令检查一下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -n kube-system | grep kube-proxy</span><br></pre></td></tr></table></figure>

<p>日志中打印出了Using ipvs Proxier，说明ipvs模式已经开启</p>
<h3><span id="5-插件安装">5 插件安装</span></h3><h5><span id="51-安装helm">5.1 安装helm</span></h5><p>Helm是k8s 的包管理器，类似于yum和apt-get，有了helm后，安装k8s的一些插件会很容易。<br>Helm由客户端命helm令行工具和服务端tiller组成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz</span><br><span class="line">tar -zxvf helm-v2.14.1-linux-amd64.tar.gz</span><br><span class="line">cd linux-amd64/</span><br><span class="line">cp helm /usr/local/bin/</span><br></pre></td></tr></table></figure>

<p>因为Kubernetes APIServer开启了RBAC访问控制，所以需要创建tiller使用的service account: tiller并分配合适的角色给它。 详细内容可以查看helm文档中的Role-based Access Control。 这里简单起见直接分配cluster-admin这个集群内置的ClusterRole给它。创建helm-rbac.yaml文件：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tiller</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cluster-admin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tiller</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="string">kubectl</span> <span class="string">create</span> <span class="string">-f</span> <span class="string">helm-rbac.yaml</span></span><br><span class="line"><span class="string">serviceaccount/tiller</span> <span class="string">created</span></span><br><span class="line"><span class="string">clusterrolebinding.rbac.authorization.k8s.io/tiller</span> <span class="string">created</span></span><br></pre></td></tr></table></figure>

<p>接下来使用helm部署tiller:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm init --service-account tiller --output yaml | sed &#x27;s@apiVersion: extensions/v1beta1@apiVersion: apps/v1@&#x27; | sed &#x27;s@ replicas: 1@ replicas: 1\n selector: &#123;&quot;matchLabels&quot;: &#123;&quot;app&quot;: &quot;helm&quot;, &quot;name&quot;: &quot;tiller&quot;&#125;&#125;@&#x27; | kubectl apply -f -</span><br></pre></td></tr></table></figure>

<p>tiller默认被部署在k8s集群中的kube-system这个namespace下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -n kube-system -l app=helm</span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">tiller-deploy-c4fd4cd68-dwkhv   1/1     Running   0          83s</span><br></pre></td></tr></table></figure>

<p>helm version</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.14.1&quot;, GitCommit:&quot;5270352a09c7e8b6e8c9593002a73535276507c0&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.14.1&quot;, GitCommit:&quot;5270352a09c7e8b6e8c9593002a73535276507c0&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>为了拉取速度快些，可以在master节点上修改helm chart仓库的地址为azure提供的镜像地址：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">helmrepo add stable http://mirror.azure.cn/kubernetes/charts</span><br><span class="line">&quot;stable&quot; has been added to your repositories</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">helm repo list</span><br><span class="line">NAME    URL                                     </span><br><span class="line">stable  http://mirror.azure.cn/kubernetes/charts</span><br><span class="line">local   http://127.0.0.1:8879/charts</span><br></pre></td></tr></table></figure>

<h5><span id="52-部署nginx-ingress">5.2 部署Nginx Ingress</span></h5><p>为了便于将集群中的服务暴露到集群外部，需要使用Ingress。</p>
<p>将master节点（node1）做为边缘节点，打上Label：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl label node node1 node-role.kubernetes.io/edge=</span><br></pre></td></tr></table></figure>

<p>stable/nginx-ingress chart的值文件ingress-nginx.yaml如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">controller:</span></span><br><span class="line">  <span class="attr">replicaCount:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">node-role.kubernetes.io/edge:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">  <span class="attr">affinity:</span></span><br><span class="line">    <span class="attr">podAntiAffinity:</span></span><br><span class="line">        <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">            <span class="attr">matchExpressions:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">              <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">              <span class="attr">values:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">nginx-ingress</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">component</span></span><br><span class="line">              <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">              <span class="attr">values:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="string">controller</span></span><br><span class="line">          <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">  <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">        <span class="attr">effect:</span> <span class="string">NoSchedule</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">        <span class="attr">effect:</span> <span class="string">PreferNoSchedule</span></span><br><span class="line"><span class="attr">defaultBackend:</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">node-role.kubernetes.io/edge:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">  <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">        <span class="attr">effect:</span> <span class="string">NoSchedule</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">        <span class="attr">effect:</span> <span class="string">PreferNoSchedule</span></span><br></pre></td></tr></table></figure>

<p>nginx ingress controller的副本数replicaCount为1，将被调度到node1这个边缘节点上。这里并没有指定nginx ingress controller service的externalIPs，而是通过hostNetwork: true设置nginx ingress controller使用宿主机网络。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">helm repo update</span><br><span class="line"></span><br><span class="line">helm install stable/nginx-ingress \</span><br><span class="line">-n nginx-ingress \</span><br><span class="line">--namespace ingress-nginx  \</span><br><span class="line">-f ingress-nginx.yaml</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>然后get pod查看下pod 状态</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -n ingress-nginx -o wide</span><br></pre></td></tr></table></figure>

<p>pod状态为running后，可以在外面访问<a target="_blank" rel="noopener" href="http://172.18.10.18返回default/">http://172.18.10.18返回default</a> backend，则部署完成。<br>5.3 安装k8s web ui （dashboard）<br>我之前使用helm安装的dashboard 1.10版本，然后很多页面都报错404 not found，后面发现是因为安装的dashboard版本有点低，不适配1.16，安装最新的v2.0.0-beta4这个版本就可以完美适配 1.16</p>
<p>安装只需要执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml</span><br></pre></td></tr></table></figure>

<p>参考下面链接中说明 创建用户 获取token， 这个教程创建的用户有最高权限<br><a target="_blank" rel="noopener" href="https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md">https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md</a></p>
<p>Create Service Account<br>We are creating Service Account with name admin-user in namespace kubernetes-dashboard first.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kubernetes-dashboard</span><br><span class="line">Create ClusterRoleBinding</span><br><span class="line">In most cases after provisioning our cluster using kops or kubeadm or any other popular tool, the ClusterRole admin-Role already exists in the cluster. We can use it and create only ClusterRoleBinding for our ServiceAccount.</span><br><span class="line">NOTE: apiVersion of ClusterRoleBinding resource may differ between Kubernetes versions. Prior to Kubernetes v1.8 the apiVersion was rbac.authorization.k8s.io/v1beta1.</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kubernetes-dashboard</span><br></pre></td></tr></table></figure>

<p>获取token:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &#x27;&#123;print $1&#125;&#x27;)</span><br></pre></td></tr></table></figure>

<p>通过ingress暴露服务：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">ingress-kube-dashboard</span></span><br><span class="line"><span class="attr">annotations:</span></span><br><span class="line"><span class="comment"># use the shared ingress-nginx</span></span><br><span class="line"><span class="attr">kubernetes.io/ingress.class:</span> <span class="string">&quot;nginx&quot;</span></span><br><span class="line"><span class="attr">nginx.ingress.kubernetes.io/ssl-redirect:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">nginx.ingress.kubernetes.io/backend-protocol:</span> <span class="string">&quot;HTTPS&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">tls:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">hosts:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">k8s.redtea.com</span></span><br><span class="line"><span class="attr">secretName:</span> <span class="string">redtea-com-tls-secret</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">host:</span> <span class="string">k8s.redtea.com</span></span><br><span class="line"><span class="attr">http:</span></span><br><span class="line"><span class="attr">paths:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">backend:</span></span><br><span class="line"><span class="attr">serviceName:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line"><span class="attr">servicePort:</span> <span class="number">443</span></span><br></pre></td></tr></table></figure>

<p>执行kubectl apply -f ingress-dashboard.yaml即可。</p>
<p>ingress用法参考： <a target="_blank" rel="noopener" href="https://qhh.me/2019/08/12/%E4%BD%BF%E7%94%A8-Kubernetes-Ingress-%E5%AF%B9%E5%A4%96%E6%9A%B4%E9%9C%B2%E6%9C%8D%E5%8A%A1/">https://qhh.me/2019/08/12/%E4%BD%BF%E7%94%A8-Kubernetes-Ingress-%E5%AF%B9%E5%A4%96%E6%9A%B4%E9%9C%B2%E6%9C%8D%E5%8A%A1/</a></p>
<p>后面就可以通过访问<br><a target="_blank" rel="noopener" href="https://k8s.redtea.com/">https://k8s.redtea.com</a><br>通过GUI 查看和管理集群。</p>
<h3><span id="6-一些注意点">6 一些注意点</span></h3><h5><span id="1-kube-proxy开启ipvs-需要加载一些内核模块在537的内核版本nf_conntrack_ipv4被nf_contrack代替所以使用下面的命令">1. kube-proxy开启ipvs 需要加载一些内核模块，在5.3.7的内核版本，nf_conntrack_ipv4被nf_contrack代替，所以使用下面的命令：</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line">#!/bin/bash</span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack</span><br><span class="line">EOF</span><br><span class="line">chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack</span><br></pre></td></tr></table></figure>

<h5><span id="2-从节点在加入主节点之前必须按照27-中的说明把swap关掉">2. 从节点在加入主节点之前，必须按照2.7 中的说明，把swap关掉</span></h5><h5><span id="3-312安装pod-network配置flannel-应该只需要在主节点执行从节点不需要">3. 3.1.2安装Pod network配置flannel 应该只需要在主节点执行，从节点不需要</span></h5><h5><span id="4-helm安装tiller报错-no-resource-found可以参考该文章解决httpsgithubcomhelmhelmissues6374使用这个命令安装helm-init-service-account-tiller-output-yaml-sed-sapiversion-extensionsv1beta1apiversion-appsv1-sed-s-replicas-1-replicas-1n-selector-matchlabels-app-helm-name-tiller-kubectl-apply-f-">4. helm安装tiller报错 no resource found，可以参考该文章解决： init –service-account tiller –output yaml | sed ‘s@apiVersion: extensions/v1beta1@apiVersion: apps/v1@’ | sed ‘s@ replicas: 1@ replicas: 1\n selector: {“matchLabels”: {“app”: “helm”, “name”: “tiller”}}@’ | kubectl apply -f -</span></h5><h5><span id="5-主节点-kubeadm-init完成后务必按照提示创建配置目录">5. 主节点 kubeadm init完成后，务必按照提示，创建配置目录：</span></h5><p>mkdir -p $HOME/.kube<br>sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<br>sudo chown $(id -u):$(id -g) $HOME/.kube/config</p>
<h5><span id="6-为了使dashboard能正常显示监控数据可以安装heapster">6. 为了使dashboard能正常显示监控数据，可以安装heapster：</span></h5><p>参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qianghaohao/article/details/98859392">https://blog.csdn.net/qianghaohao/article/details/98859392</a></p>
<p>一些机器原始设置备份：<br>/etc/default/grub 原始配置备份：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GRUB_TIMEOUT=5</span><br><span class="line">GRUB_DISTRIBUTOR=&quot;$(sed &#x27;s, release .*$,,g&#x27; /etc/system-release)&quot;</span><br><span class="line">GRUB_DEFAULT=saved</span><br><span class="line">GRUB_DISABLE_SUBMENU=true</span><br><span class="line">GRUB_TERMINAL_OUTPUT=&quot;console&quot;</span><br><span class="line">GRUB_CMDLINE_LINUX=&quot;crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet&quot;</span><br><span class="line">GRUB_DISABLE_RECOVERY=&quot;true&quot;</span><br></pre></td></tr></table></figure>




<p>其他：</p>
<p>关于ingress ngnix <a target="_blank" rel="noopener" href="https://qhh.me/2019/08/12/%E4%BD%BF%E7%94%A8-Kubernetes-Ingress-%E5%AF%B9%E5%A4%96%E6%9A%B4%E9%9C%B2%E6%9C%8D%E5%8A%A1/">https://qhh.me/2019/08/12/%E4%BD%BF%E7%94%A8-Kubernetes-Ingress-%E5%AF%B9%E5%A4%96%E6%9A%B4%E9%9C%B2%E6%9C%8D%E5%8A%A1/</a></p>
<p>获取dashboard登录token：<br>kubectl -n kube-system get secret | grep kubernetes-dashboard-token<br>kubernetes-dashboard-token-xmzng<br>kubectl describe -n kube-system secret/kubernetes-dashboard-token-xmzng</p>
<p>kubectl config set-context kubernetes-dashboard@kubernetes –cluster=kubernetes –user=kubernetes-dashboard –kubeconfig=/root/dashbord-admin.conf<br>kubectl config user-context kubernetes-dashboard@kubernets –kubeconfig=/root/dashbord-admin.conf</p>
<p>上面搭的集群不具备高可用的特性，只有一个master节点，一旦该master挂掉，集群就无法管理了<br>关于高可用k8s集群：<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/networken/article/details/89599004">https://blog.csdn.net/networken/article/details/89599004</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/fanren224/article/details/86573264">https://blog.csdn.net/fanren224/article/details/86573264</a><br><a target="_blank" rel="noopener" href="https://blog.51cto.com/billy98/2350660">https://blog.51cto.com/billy98/2350660</a><br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/8eb81d1674dc">https://www.jianshu.com/p/8eb81d1674dc</a><br><a target="_blank" rel="noopener" href="https://www.kubernetes.org.cn/5273.html">https://www.kubernetes.org.cn/5273.html</a></p>
<p>参考：<br>使用kubeadm部署k8s集群： <a target="_blank" rel="noopener" href="https://www.kubernetes.org.cn/5551.html">https://www.kubernetes.org.cn/5551.html</a><br><a target="_blank" rel="noopener" href="https://www.kubernetes.org.cn/5551.html">https://www.kubernetes.org.cn/5551.html</a></p>
</div><div class="tags"><a href="/tags/%E7%AC%94%E8%AE%B0/"><i class="fa fa-tag"></i>笔记</a></div><div class="post-nav"><a class="next" href="/2020/02/01/distribute_lock_redisson/">关于Redis分布式锁以及Redisson的用法</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == 'true' ? true : false;
var verify = 'false' == 'true' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'phSM9k9SgjYgD57rnCmYul5b-MdYXbMMI',
  appKey:'J2I1qMXMAdlNQtczKNTcesXL',
  placeholder:'Just so so',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})
</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://example.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Emotion/">Emotion</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Poetry/">Poetry</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Technology/">Technology</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Travel/">Travel</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%91%84%E5%BD%B1/">摄影</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E6%97%85%E8%A1%8C/" style="font-size: 15px;">旅行</a> <a href="/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 15px;">笔记</a> <a href="/tags/%E8%AE%A1%E5%88%92/" style="font-size: 15px;">计划</a> <a href="/tags/Deepin/" style="font-size: 15px;">Deepin</a> <a href="/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/tags/JAVA/" style="font-size: 15px;">JAVA</a> <a href="/tags/JAVA-%E5%B9%B6%E5%8F%91/" style="font-size: 15px;">JAVA 并发</a> <a href="/tags/Mybatis/" style="font-size: 15px;">Mybatis</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Redis/" style="font-size: 15px;">Redis</a> <a href="/tags/Spring/" style="font-size: 15px;">Spring</a> <a href="/tags/DesignPattern/" style="font-size: 15px;">DesignPattern</a> <a href="/tags/%E6%96%AD%E8%88%8D%E7%A6%BB/" style="font-size: 15px;">断舍离</a> <a href="/tags/%E5%BE%92%E6%AD%A5/" style="font-size: 15px;">徒步</a> <a href="/tags/%E6%A1%86%E6%9E%B6/" style="font-size: 15px;">框架</a> <a href="/tags/%E6%91%84%E5%BD%B1-%E4%BA%BA%E6%96%87/" style="font-size: 15px;">摄影 人文</a> <a href="/tags/%E6%97%85%E8%A1%8C-%E8%A5%BF%E8%97%8F-%E6%91%84%E5%BD%B1/" style="font-size: 15px;">旅行 西藏 摄影</a> <a href="/tags/%E8%AF%97/" style="font-size: 15px;">诗</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/02/01/k8s_cluster_deploy_116/">基于kubeadm搭建k8s集群v1.16</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/01/distribute_lock_redisson/">关于Redis分布式锁以及Redisson的用法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/01/redis_lua_counter/">使用redistemplate调用lua脚本实现的redis计数器</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/01/encrypt_alg/">说说加密算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/01/imsi_iccid_imei/">电信领域几个名词的理解(IMSI ICCID IMEI)</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/01/k8s_meter_unit/">Kubernetes中的计量单位</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/01/mybatis_batch_oper/">Mybatis批量insert数据的写法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/01/2020/">不可能完成的年度计划之2020篇</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/20/CI_CD_microservice/">对CI CD 微服务的一些理解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/10/19/loadaverage/">关于load average 指标</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://weibo.com/EricZhaoly/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo&amp;is_all=1" title="微博" target="_blank">微博</a><ul></ul><a href="https://github.com/zzjeric" title="Github" target="_blank">Github</a><ul></ul><a href="https://www.zhihu.com/people/zhao-zi-shang" title="知乎" target="_blank">知乎</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">Eric's blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/love.js"></script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>